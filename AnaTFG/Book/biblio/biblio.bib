@article{Saeys2007,
abstract = {Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.},
author = {Saeys, Yvan and Inza, I{\~{n}}aki and Larra{\~{n}}aga, Pedro},
doi = {10.1093/bioinformatics/btm344},
isbn = {1367-4803, 1460-2059},
issn = {1367-4803, 1460-2059},
journal = {Bioinformatics},
number = {19},
pages = {2507--2517},
title = {A review of feature selection techniques in Bioinformatics},
volume = {23},
year = {2007}
}

@article{Kalousis2007,
author = {Kalousis, Alexandros and Prados, Julien and Hilario, Melanie},
year = {2007},
month = {05},
pages = {95-116},
title = {Stability of feature selection algorithms: A study on high-dimensional spaces},
volume = {12},
journal = {Knowl. Inf. Syst.},
doi = {10.1007/s10115-006-0040-8}
}

@article{JMLR2018,
author = {Nogueira, Sarah and Sechidis, Konstantinos and Brown, G.},
year = {2018},
month = {04},
pages = {1-54},
title = {On the stability of feature selection algorithms},
volume = {18},
journal = {Journal of Machine Learning Research}
}

@article{CANO20134820,
title = {Analysis of data complexity measures for classification},
journal = {Expert Systems with Applications},
volume = {40},
number = {12},
pages = {4820-4831},
year = {2013},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2013.02.025},
url = {https://www.sciencedirect.com/science/article/pii/S0957417413001413},
author = {José-Ramón Cano},
keywords = {Data complexity, Class overlapping, Class separability, Classification},
abstract = {The study of data complexity metrics is an emergent area in the field of data mining and is focused on the analysis of several data set characteristics to extract knowledge from them. This information can be used to support the election of the proper classification algorithm. This paper addresses the analysis of the relationship between data complexity measures and classifiers behavior. Each one of the metrics is evaluated covering its range of values and studying the classifiers accuracy on these values. The results offer information about the usefullness of these measures, and which of them allow us to analyze the nature of the input data set and help us to decide which classification method could be the most promising one.}
}

@Misc{rstudio,
  note =	 {\url{https://www.rstudio.com/products/rstudio/} [Retrieved 3 June 2022]},
  title =	 {RStudio installation web}
}

@Misc{nogueirs,
  note =	 {\url{https://github.com/nogueirs/JMLR2018} [Retrieved  3 June 2022]},
  title =	 {GitHub repository of JMLR2018 article}
}

@Misc{rmarkdown,
  note =	 {\url{https://rmarkdown.rstudio.com/} [Retrieved 3 June 2022]},
  title =	 {R Markdown from RStudio}
}

@Misc{r-project,
  note =	 {\url{https://www.r-project.org/} [Retrieved 3 June 2022]},
  title =	 {The R Project for Statistical Computing}
}

@Misc{overleaf,
  note =	 {\url{https://www.overleaf.com/about} [Retrieved 3 June 2022]},
  title =	 {Overleaf - About us}
}

@inproceedings{Fayyad96b,
  added-at = {2017-01-04T13:05:56.000+0100},
  author = {Fayyad, U. M. and Piatetsky-Shapiro, G. and Smyth, P.},
  biburl = {https://www.bibsonomy.org/bibtex/2842742fc1e7285bf86bb75a056472e05/wipster},
  booktitle = {Proceedings of the 2nd international conference on Knowledge Discovery    and Data mining (KDD'96)},
  interhash = {5b2c70bce9a6653f712b078320adba3b},
  intrahash = {842742fc1e7285bf86bb75a056472e05},
  keywords = {data discovery knowledge masterarbeit mining},
  month = {August},
  pages = {82--88},
  publisher = {AAAI Press},
  timestamp = {2017-01-04T13:05:56.000+0100},
  title = {Knowledge discovery and data mining : Towards a unifying framework},
  year = 1996
}

@Misc{high-dimensionality,
  note =	 {\url{https://www.statology.org/high-dimensional-data/} [Retrieved 25 August 2022]},
  title =	 {What is High Dimensional Data? (Definition \& Examples)},
  author = {ZACH}
}

@article{carr1999strategically,
  title={Strategically managed buyer--supplier relationships and performance outcomes},
  author={Carr, Amelia S and Pearson, John N},
  journal={Journal of operations management},
  volume={17},
  number={5},
  pages={497--519},
  year={1999},
  publisher={Elsevier}
}

@Misc{data-mining,
  note =	 {\url{https://www.ibm.com/cloud/learn/data-mining} [Retrieved 26 August 2022]},
  title =	 {Data Mining},
  author = {IBM Cloud Education},
  howpublished = {15 January 2021}
}

@incollection{Hernandez-Orallo05a,
  author    = {Jos{\'{e}} Hern{\'{a}}ndez{-}Orallo},
  editor    = {Laura C. Rivero and
            Jorge Horacio Doorn and
            Viviana E. Ferraggine},
  title     = {Knowledge Discovery from Databases},
  booktitle = {Encyclopedia of Database Technologies and Applications},
  pages     = {313--318},
  publisher = {Idea Group},
  year      = {2005},
  url       = {https://doi.org/10.4018/978-1-59140-560-3.ch054},
  doi       = {10.4018/978-1-59140-560-3.ch054},
  timestamp = {Thu, 27 Jun 2019 15:50:12 +0200},
  biburl    = {https://dblp.org/rec/books/idea/encyclopediaDB2005/Hernandez-Orallo05a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@Misc{visual-crisp-dm,
  note =	 {\url{https://exde.wordpress.com/2009/03/13/a-visual-guide-to-crisp-dm-methodology/} [Retrieved 29 August 2022]},
  title =	 {a visual guide to crisp-dm methodology},
  author = {Nicole Leaper}
}

@Misc{preprocessing-time,
  note =	 {\url{https://www.ibm.com/docs/en/spss-modeler/saas?topic=preparation-data-overview} [Retrieved 30 August 2022]},
  title =	 {Data Preparation Overview},
  author = {IBM Corporation},
  howpublished = {17 August 2021}
}

@Misc{supervised-learning,
  note =	 {\url{https://www.ibm.com/cloud/learn/supervised-learning} [Retrieved 1 September 2022]},
  title =	 {Supervised Learning},
  author = {IBM Cloud Education},
  howpublished = {19 August 2020}
}

@Misc{unsupervised-learning,
  note =	 {\url{https://www.ibm.com/cloud/learn/unsupervised-learning} [Retrieved 1 September 2022]},
  title =	 {Unupervised Learning},
  author = {IBM Cloud Education},
  howpublished = {21 September 2020}
}

@Misc{fs-method-ml,
  note =	 {\url{https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/} [Retrieved 2 September 2022]},
  title =	 {How to Choose a Feature Selection Method For Machine Learning},
  author = {Jason Brownlee},
  howpublished = {November 27, 2019}
}

@Misc{dimensionality-reduction,
  note =	 {\url{https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/} [Retrieved 5 September 2022]},
  title =	 {Introduction to Dimensionality Reduction for Machine Learning},
  author = {Jason Brownlee},
  howpublished = {May 6, 2020}
}

@article{GuyonElisseeff2003,
author = {Guyon, Isabelle and Elisseeff, André},
year = {2003},
month = {01},
pages = {1157 - 1182},
title = {An Introduction of Variable and Feature Selection},
volume = {3},
journal = {J. Machine Learning Research Special Issue on Variable and Feature Selection},
doi = {10.1162/153244303322753616}
}

@article{liu1998feature,
  title={Feature transformation and subset selection},
  author={Liu, Huan and Motoda, Hiroshi},
  journal={IEEE Intell Syst Their Appl},
  volume={13},
  number={2},
  pages={26--28},
  year={1998}
}

@ARTICLE{optimal,
  author={Halliday, Daniel L. and McGillem, Clare D. and Westerkamp, John and Aunon, Jorge I.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics}, 
  title={Optimal and suboptimal feature selection for classification of evoked brain potentials}, 
  year={1985},
  volume={SMC-15},
  number={3},
  pages={442-448},
  abstract={Exhaustive feature selection algorithms are optimal because all possible combinations of features are tested against a predetermined criterion. Suboptimal algorithms that trade performance for speed by considering only a subset of all feature combinations are generally preferred. An implementation of the exhaustive search feature selection (ESFS) method is described for the Bayes Gaussian statistics. The algorithm significantly reduces the computational and time requirements normally associated with optimal algorithms. The performance of this algorithm is compared to that of two suboptimal algorithms-forward sequential features selection and stepwise linear discriminant analysis. Results show that this implementation provides a moderate improvement in classification accuracy and is well suited for evaluating the performance of suboptimal algorithms.},
  keywords={},
  doi={10.1109/TSMC.1985.6313381},
  ISSN={2168-2909},
  month={May}
}

@article{metrics2013,
author = {Brij Mohan Goel and Prof. Pradeep Kumar Bhatia},
year = {2013},
month = {01},
pages = {18-27},
title = {An Overview of Various Object Oriented Metrics},
volume = {2},
journal = {International Journal of Information Technology and Systems (IJITS) - ISSN 2277–9825}
}

@Article{fsinr,
    title = {FSinR: an exhaustive package for feature selection},
    author = {Francisco Aragón-Royón and Alfonso Jiménez-Vílchez and
      Antonio Arauzo-Azofra and José Manuel Benitez},
    journal = {arXiv e-prints},
    keywords = {Computer Science - Machine Learning, Statistics -
      Machine Learning},
    year = {2020},
    month = {feb},
    eid = {arXiv:2002.10330},
    pages = {arXiv:2002.10330},
    archiveprefix = {arXiv},
    eprint = {2002.10330},
    primaryclass = {cs.LG},
    url = {https://arxiv.org/abs/2002.10330},
}

@Manual{caret,
    title = {caret: Classification and Regression Training},
    author = {Max Kuhn},
    year = {2022},
    note = {R package version 6.0-93},
    url = {https://CRAN.R-project.org/package=caret},
  }

@Misc{fs-numerical,
  note =	 {\url{https://machinelearningmastery.com/feature-selection-with-numerical-input-data/} [Retrieved 13 September 2022]},
  title =	 {How to Perform Feature Selection With Numerical Input Data},
  author = {Jason Brownlee},
  howpublished = {August 18, 2020}
}

@Article{randomForest,
    title = {Classification and Regression by randomForest},
    author = {Andy Liaw and Matthew Wiener},
    journal = {R News},
    year = {2002},
    volume = {2},
    number = {3},
    pages = {18-22},
    url = {https://CRAN.R-project.org/doc/Rnews/},
}

@Article{Boruta,
    title = {Feature Selection with the {Boruta} Package},
    author = {Miron B. Kursa and Witold R. Rudnicki},
    journal = {Journal of Statistical Software},
    year = {2010},
    volume = {36},
    number = {11},
    pages = {1--13},
    url = {http://www.jstatsoft.org/v36/i11/},
}
  
@Misc{fs-caret,
  note =	 {\url{https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/} [Retrieved 15 September 2022]},
  title =	 {Feature Selection with the Caret R Package},
  author = {Jason Brownlee},
  howpublished = {September 22, 2014}
}

@Manual{spfsr,
    title = {spFSR: Feature Selection and Ranking by Simultaneous Perturbation Stochastic Approximation},
    author = {Vural Aksakalli and Babak Abbasi and Yong Kai Wong},
    year = {2018},
    note = {R package version 1.0.0},
    url = {https://CRAN.R-project.org/package=spFSR},
}

@Article{mlr,
    title = {{mlr}: Machine Learning in R},
    author = {Bernd Bischl and Michel Lang and Lars Kotthoff and Julia
      Schiffner and Jakob Richter and Erich Studerus and Giuseppe
      Casalicchio and Zachary M. Jones},
    journal = {Journal of Machine Learning Research},
    year = {2016},
    volume = {17},
    number = {170},
    pages = {1-5},
    url = {https://jmlr.org/papers/v17/15-066.html},
}

@Manual{varselrf,
    title = {varSelRF: Variable Selection using Random Forests},
    author = {Ramon Diaz-Uriarte},
    year = {2017},
    note = {R package version 0.7-8},
    url = {https://CRAN.R-project.org/package=varSelRF},
}

@article{ecol-article,
author = {Lorena, Ana and Garcia, Luís Paulo and Lehmann, Jens and de Souto, Marcilio and Ho, Tin},
year = {2019},
month = {09},
pages = {1-34},
title = {How Complex Is Your Classification Problem?: A Survey on Measuring Classification Complexity},
volume = {52},
journal = {ACM Computing Surveys},
doi = {10.1145/3347711}
}

@ARTICLE{TinKamHoBasu2002,
    author={Tin Kam Ho and Basu, M.},
    journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
    title={Complexity measures of supervised classification problems},
    year={2002},
    volume={24},
    number={3},
    pages={289-300},
    abstract={We studied a number of measures that characterize the difficulty of a classification problem, focusing on the geometrical complexity of the class boundary. We compared a set of real-world problems to random labelings of points and found that real problems contain structures in this measurement space that are significantly different from the random sets. Distributions of problems in this space show that there exist at least two independent factors affecting a problem's difficulty. We suggest using this space to describe a classifier's domain of competence. This can guide static and dynamic selection of classifiers for specific problems as well as subproblems formed by confinement, projection, and transformations of the feature vectors.},
    keywords={},
    doi={10.1109/34.990132},
    ISSN={1939-3539},
    month={March}
}

@ARTICLE{LiSiZhou2015,
    author={Li, Yun and Si, Jennie and Zhou, Guojing and Huang, Shasha and Chen, Songcan},
    journal={IEEE Transactions on Neural Networks and Learning Systems},
    title={FREL: A Stable Feature Selection Algorithm},   year={2015},
    volume={26},
    number={7},
    pages={1388-1402},
    abstract={Two factors characterize a good feature selection algorithm: its accuracy and stability. This paper aims at introducing a new approach to stable feature selection algorithms. The innovation of this paper centers on a class of stable feature selection algorithms called feature weighting as regularized energy-based learning (FREL). Stability properties of FREL using L1 or L2 regularization are investigated. In addition, as a commonly adopted implementation strategy for enhanced stability, an ensemble FREL is proposed. A stability bound for the ensemble FREL is also presented. Our experiments using open source real microarray data, which are challenging high dimensionality small sample size problems demonstrate that our proposed ensemble FREL is not only stable but also achieves better or comparable accuracy than some other popular stable feature weighting methods.},  keywords={},
    doi={10.1109/TNNLS.2014.2341627},
    ISSN={2162-2388},
    month={July}
}

@article{shap,
author = {Iñaki Inza and Borja Calvo},
title = {Visualización de datos con R},
journal = {Universidad del País Vasco},
year = {Master ICSI-KISA}
}